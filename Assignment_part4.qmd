---
title: | 
  | Econometrics III 
  | Assignment Part IV
author: | 
  | Thao Le (523716) 
  | David Gyaraki (582340)
format: pdf
pdf:
  documentclass: article
  cite-method: biblatex
editor: source
include-in-header:
  text: |
    \addtokomafont{disposition}{\rmfamily}
    \usepackage{amsmath}
    \newcommand{\bm}{\symbf}
    \newcommand{\T}{\text{T}}
    \newcommand{\pl}{\text{plim}}
    \newcommand{\brefsection}[1]{Section \textcolor{blue}{\ref{#1}}}
    \newcommand{\beqref}[1]{Equation \textcolor{blue}{\eqref{#1}}}
    \newcommand{\breftable}[1]{Table \textcolor{blue}{\ref{#1}}}
    \newcommand{\breffig}[1]{Figure \textcolor{blue}{\ref{#1}}}
    \usepackage{fancyvrb}
    \usepackage{dcolumn}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
pdf-engine: xelatex
cap-location: top
toc: true
toc-title: Contents
number-sections: true
mainfont: Arial
setspace:
  linestretch: 1.25
fig-align: center
table-align: center
fig-pos: H
table-pos: H
execute:
  echo: true
  warning: false
  eval: true
code-line-numbers: false
colorlinks: true
code-block-bg: darkgray
df-print: default
highlight-style: arrow-dark
biblio-title: References
---

\clearpage

```{r setup part1, echo=TRUE, results='hide'}
# load packages
if(!require(pacman)){install.packages("pacman")}

p_load(devtools,tidyverse,dplyr,ggplot2,latex2exp,stargazer, fixest, modelsummary, knitr, readr, tseries, lmtest, forecast, dynlm, vars, xtable)

dfAssign_p4 <- as.data.frame(read_csv("data/data_assign_p4.csv"))

# Encode quarters
dfAssign_p4 <- cbind(dfAssign_p4,c(seq(1,nrow(dfAssign_p4),length.out=nrow(dfAssign_p4))))
colnames(dfAssign_p4) <- c("obs", "CONS", "INC", "TIME")
```

We are always more interested in estimating ADL(1,1) instead of 1st diff ADL(1,1) because we are not able to say anything about the long run equilibrium in case the 1st diff are used

\section{Question 1} \label{q1}

```{r q1}
# Set the parameters
gamma <- 0.5
sigma_u <- 1
sigma_v <- 1
phi <- 0.9
init_step = 200
max_T = 2000

# Function to simulate all data
sim_data <- function(T) {
  vt <- rnorm(T, mean = 0, sd = sqrt(sigma_v))
  ut <- rnorm(T, mean = 0, sd = sqrt(sigma_u))
  
  #X_t and Y_t in the cointergrated model
  xt_cointergrated <- cumsum(vt) 
  yt_cointergrated <- gamma * xt_cointergrated + ut
  
  #X_t and Y_t in the stationary model
  xt_staionary <- numeric(T)
  xt_staionary[1] <- vt[1]
  for (t in 2:T) {
    xt_staionary[t] <- phi * xt_staionary[t-1] + vt[t]
  }
  yt_staionary <- gamma * xt_staionary + ut
  
  return(list(xt_cointergrated = xt_cointergrated, yt_cointergrated = yt_cointergrated,
              xt_staionary = xt_staionary, yt_staionary = yt_staionary))
}

# Initialize dataframe to store results
sim_results <- data.frame(
  T = numeric(),
  beta_hat_1 = numeric(),
  t_stat_1 = numeric(),
  R_squared_1 = numeric(),
  beta_hat_2 = numeric(),
  t_stat_2 = numeric(),
  R_squared_2 = numeric(),
  stringsAsFactors = FALSE
)

# For each sample size T from T = 200,400,..., 1000
for (T in seq(init_step, max_T, 200)) {
  lData = sim_data(T)
  
  # Cointegrated series
  X1_t = lData$xt_cointergrated
  Y1_t = lData$yt_cointergrated
  
  # Stationary series
  X2_t = lData$xt_staionary
  Y2_t = lData$yt_staionary  
    
  # Perform linear regression and get summary stats on both models
  model1 <- lm(Y1_t ~ X1_t)
  summary1 <- tidy(model1)
  beta_hat1 <- summary1$estimate[2]
  t_stat1 <- summary1$statistic[2]
  R_squared1 <- summary(model1)$r.squared
  
  model2 <- lm(Y2_t ~ X2_t)
  summary2 <- tidy(model2)
  beta_hat2 <- summary2$estimate[2]
  t_stat2 <- summary2$statistic[2]
  R_squared2 <- summary(model2)$r.squared
  
  # Store results
  sim_results <- sim_results %>% 
    add_row(T = T,
          beta_hat_1 = beta_hat1,
          t_stat_1 = t_stat1,
          R_squared_1 = R_squared1,
          beta_hat_2 = beta_hat2,
          t_stat_2 = t_stat2,
          R_squared_2 = R_squared2,
          )
}
```

\begin{table}[H]
\centering
\begin{tabular}{rrrrrrrr}
  \hline
 & T & beta\_hat\_1 & t\_stat\_1 & R\_squared\_1 & beta\_hat\_2 & t\_stat\_2 & R\_squared\_2 \\ 
  \hline
1 & 200 & 0.54 & 24.62 & 0.75 & 0.51 & 13.08 & 0.46 \\ 
  2 & 400 & 0.50 & 75.90 & 0.94 & 0.49 & 21.06 & 0.53 \\ 
  3 & 600 & 0.50 & 85.59 & 0.92 & 0.49 & 25.48 & 0.52 \\ 
  4 & 800 & 0.50 & 124.18 & 0.95 & 0.51 & 33.60 & 0.59 \\ 
  5 & 1000 & 0.50 & 137.93 & 0.95 & 0.49 & 32.81 & 0.52 \\ 
  6 & 1200 & 0.50 & 211.06 & 0.97 & 0.50 & 40.04 & 0.57 \\ 
  7 & 1400 & 0.50 & 553.08 & 1.00 & 0.50 & 39.90 & 0.53 \\ 
  8 & 1600 & 0.50 & 244.21 & 0.97 & 0.49 & 43.78 & 0.55 \\ 
  9 & 1800 & 0.50 & 255.31 & 0.97 & 0.51 & 48.00 & 0.56 \\ 
  10 & 2000 & 0.50 & 415.59 & 0.99 & 0.51 & 51.27 & 0.57 \\ 
   \hline
\end{tabular}
\end{table}
Looking at the results of model 1 (using non-stationary cointegrated variables), we can see that the $\hat\beta$ estimates are quite reliable and consistent, with higher T-statistics and higher R-squared scores. This indicates that we did not run into the spurious regression problems. On the other hand, the result of model 2 suggests that the model perform poorly, with lower T-statistics and low R-squared scores suggest that the model perform poorly and the results are not reliable. This suggests that we run into the spurious regression problems.
\section{Question 2} \label{q2}

```{r q2}
plot.ts(dfAssign_p4$CONS, main="Quarterly Aggregate Consumption since 1988 Q1", ylab="Aggregate Consumption")

plot.ts(dfAssign_p4$INC, main="Quarterly Aggregate Income since 1988 Q1", ylab="Aggregate Income")
```

The two plots above show the shape of the quarterly aggregate consumption and quarterly aggregate income in the Netherlands. From this we can see that due to the aggregate nature of the two series, the series are not likely to be stationary at first glance.


```{r q2 acf}
acf(dfAssign_p4$CONS,12,pl=T, main="ACF of the Aggregate Consumption")

acf(dfAssign_p4$INC,12,pl=T, main="ACF of the Aggregate Income")

pacf(dfAssign_p4$CONS,12,pl=T, main="PACF of the Aggregate Consumption")

pacf(dfAssign_p4$INC,12,pl=T, main="PACF of the Aggregate Income")
```

From the ACF and PACF plots for the consumption and income components of the GDP, we can see that due to the aggregate nature of the series, both the consumption and income autocorrelations show significant correlation (above the white-noise threshold) across all lags all the way to the 12th period lag. However, when we investigate the partial autocorrelation function results, both variables have insignificant correlations for all lags (1st to 12th). While some correlations are more pronounced for consumption than for income, no values reach the white-noise threshold and therefore are not very significant.

\section{Question 3} \label{q3}

```{r q3}
adf.test(dfAssign_p4$CONS)

adf.test(dfAssign_p4$INC)
```

Based on the test results, we cannot reject in either case that the series would be non-stationary. Since we do not find sufficient evidence for the stationarity, we can stay at the null-hypothesis of the series being non-stationary. 

\section{Question 4} \label{q4}

```{r q4}
d1_CONS <- diff(dfAssign_p4$CONS, differences = 1)

d1_INC <- diff(dfAssign_p4$INC, differences = 1)

adf.test(d1_CONS)

adf.test(d1_INC)
```

In the case of the first-difference series, we still cannot reject the null hypothesis on the first case (consumption), however the second Augmented Dickey-Fuller test has a p-value of $p= 2.71\%$ and hence can be rejected at a 5\% significance level. This means that the first difference of the income component of the GDP might be stationary.

order of cointegration?

\section{Question 5} \label{q5}

```{r q5}

```

\section{Question 6} \label{q6}

```{r q6}

```

\section{Question 7} \label{q7}

```{r q7}

```

\section{Question 8} \label{q8}

```{r q8}

```
